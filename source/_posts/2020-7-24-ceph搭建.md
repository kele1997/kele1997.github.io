---
title: ceph 集群搭建
date: 2020-07-24 12:49:37
tags:
---
# 一、部署监控节点  ceph-mon
启动监控节点需要下面这些内容：  
1. unique id ==> fsid 集群唯一标识 id,区别集群   
2. cluster name ==> 集群名（不可以有空格）  默认为 ceph, 多个集群时，集群名便于查看   `ceph --cluster {cluster-name}).` ，也可以简化命令行操作 
3. monitor name ==> 监控实例的名字，普遍设置为监控实例所在主机的主机名，（推荐一个主机只有一个监控，最好不要有 osd 和其他监控)  
4. monitor map ==> 启动时需要生成，需要 fsid， cluster name ,至少一个名字或者IP地址(大概意思时至少有一个节点吧）  
5. administrator keyring ==> ceph cli 需要有 client.admin 用户，必须生成 admin 用户和 keyring 文件，添加 client.admin 用户 到 monitor keyring 中  

来源： https://docs.ceph.com/docs/master/install/manual-deployment/






## 1. 安装必要的软件包
监控节点需要安装 ceph-common ceph-base ceph-mon librados2 等包
`yum install ceph-common ceph-base ceph-mon librados2` 




## 2. 生成 fsid，修改配置文件
```
# 这条命令会生成一个 uuid
uuidgen
```


`vim /etc/ceph/ceph.conf` 
修改配置文件如下   


```
[global]
# 上面使用 uuidgen 生成的 uuid
fsid = a7f64266-0894-4f1e-a635-d0aeaca0e993
# cloud1指的是监控节点的主机名,如果多个监控节点，使用逗号分隔
mon initial members = cloud1[,<hostname>]
# 监控节点的 ip 地址，注意是 ip 地址,多个监控节点，使用逗号分隔
mon host = 172.16.0.1,[, <hostip>]
# 管理网网络
public network = 172.16.0.0/24
# 使用 cephx 认证
auth cluster required = cephx
auth service required = cephx
auth client required = cephx
osd journal size = 1024
# 默认 object store pool size 
osd pool default size = 3 
# osd pool 的最小要求，小于2个 osd 无法 warn
osd pool default min size = 2
osd pool default pg num = 333
osd pool default pgp num = 333
osd crush chooseleaf type = 1
```


## 3. 生成 keyring 密钥,启动ceph-mon服务

### 3.1. 创建一个 keyring 文件
**注意，这行命令中的 `mon.` 后面有一个 **`.`**,不要漏掉
这条命令会在 /tmp/ceph 下生成一个密钥文件，并且允许一切 ceph 一切权限
`ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *'`


### 3.2. 创建 administrator keyring
生成一个 administrator 最高管理员的密钥，并允许一切 ceph 权限
`ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *'`


### 3.3. 创建一个 bootstrap-osd keyring 
生成一个允许启动 osd 的密钥
`ceph-authtool --create-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring --gen-key -n client.bootstrap-osd --cap mon 'profile bootstrap-osd' --cap mgr 'allow r'`



### 3.4. 把密钥添加到 ceph.mon.keyring 中，以允许监控服务拥有上述用户权限   
将admin 的密钥和bootstrap-osd 的密钥都导入到mon的密钥中
```
ceph-authtool /tmp/ceph.mon.keyring --import-keyring etc/ceph/ceph.client.admin.keyring
ceph-authtool /tmp/ceph.mon.keyring --import-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring
```


### 3.5. 修改 ceph.mon.keyring 文件所有者及所有者组（因为ceph以ceph用户运行，所以要保证有读取权限）
`chown ceph:ceph /tmp/ceph.mon.keyring`


### 3.6. 生成 monmap 文件
hostname 本机的主机名
ip-address 是本机的管理网地址
uuid 是 `/etc/ceph/ceph.conf` 中的fsid
`monmaptool --create --add {hostname} {ip-address} --fsid {uuid} /tmp/monmap`  


### 3.7. 创建一个 ceph-mon 工作目录
**注意使用 `sudo -u ceph` 切换到 ceph用户执行,否则需要手动 chown -R ceph:ceph <filename> 修改文件夹所有者**
如果多级目录不存在，可以考虑为 mkdir 传递 `-p` 参数    
这里的 `cluster-name` 使用 `ceph`, 如果有多个ceph集群，可以使用多个不同的集群名  
hostname 即本机主机名  
`sudo -u ceph mkdir /var/lib/ceph/mon/{cluster-name}-{hostname}`   
示例：
`sudo -u ceph mkdir /var/lib/ceph/mon/ceph-cloud1` 


### 3.8. 创建新的 monitor fs
这里的集群名属于可选参数，因为一般都是一套集群，这里的集群名是 ceph  
`sudo -u ceph ceph-mon [--cluster {cluster-name}] --mkfs -i {hostname} --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring`  

示例：  
`sudo -u ceph ceph-mon --cluster ceph --mkfs -i cloud1 --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring`  

### 3.9. 启动 ceph-mon 
`systemctl start ceph-mon@cloud1` # cloud1 为主机名


### 3.10 验证 ceph-mon 是否正常 
ceph -s
输出为：
有输出则表示 ceph-mon 服务已经启动
```
  cluster:
    id:     a31d0c20-a7f0-43ab-92ca-8a4a7d88ef68
    health: HEALTH_WARN
            1 monitors have not enabled msgr2
 
  services:
    mon: 1 daemons, quorum cloud1 (age 15m)
    mgr: no daemons active
    osd: 0 osds: 0 up, 0 in
 
  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:     


```
## 4. 启用 messager v2 
https://docs.ceph.com/docs/master/rados/configuration/msgr2/
在监控节点上运行 `ceph mon enable-msgr2` 

使用 `ceph -s` 查看集群状态,不再提示要启用 msgr2 
```
[root@cloud1 ~]# ceph -s
  cluster:
    id:     a31d0c20-a7f0-43ab-92ca-8a4a7d88ef68
    health: HEALTH_OK
 
  services:
    mon: 1 daemons, quorum cloud1 (age 2m)
    mgr: cloud1(active, since 17h)
    osd: 3 osds: 3 up (since 18h), 3 in (since 18h)
 
  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   3.0 GiB used, 279 GiB / 282 GiB avail
    pgs:     
 
```


# 二、 部署管理节点 ceph-mgr
** 在每一台部署了 ceph-mon 的机器上，都应该安装 ceph-mgr 服务 **  
首先安装包:   
`yum install ceph-common ceph-mon`   

## 1. 添加 mgr 权限
```
#这里的 $hostname 使用一个名字代替，这里使用主机名 cloud1
ceph auth get-or-create mgr.$hostname mon 'allow profile mgr' osd 'allow *' mds 'allow *'

```
输出类似如下：   
```
[mgr.cloud1]
    key = AQCZ5QZfYap4HRAA97gVwodPmJB7dhZzbDYNpA==
```
将上述内容复制到 `/var/lib/ceph/mgr/{cluster-name}-{$name}` 文件中
`cluster-name` 一般是`ceph`, $hostname 是主机名   


## 2. 启动 mgr 服务
`ceph-mgr -i ${hostname} # hostname 指主机名`
PS: 关闭可以使用 `pkill ceph-mgr`

TODO: 貌似有部分高可用的功能




# 三、部署存储节点 ceph-osd
mon 服务和 mgr 服务都部署完成之后，就可以部署 osd 存储服务了，这个最好是和监控节点分离开，不要部署再同一台主机上  
osd 的存储后端有两种： bluestore(据说性能比较好) filestore
`bluestore` ==> 数据直接写入磁盘，磁盘上没有文件系统
`filestore` ==> 数据写入到磁盘中的文件系统中，数据直接以文件的形式存在 

这里选用 `bluestore` 进行存储部署  

首先安装软件包  
`yum install ceph-common ceph-base ceph-osd python3-rbd ceph-common`
ceph 的客户端需要安装 ceph client packge   
如果完整安装完成之后会有一个 `ceph-volume` 的命令   

```
# 这一步需要一个空的分区
# 将监控节点的配置文件 /etc/ceph/ceph.conf 和 osd 密钥 /var/lib/ceph/bootstrap-osd/ceph.keyring 复制到本机对应的目录
# 这一步是为了让本机能够通过配置文件知道监控节点的地址，以及通过密钥与监控节点进行通信
# 之后通过 ceph-volume 创建lvm卷，并自动创建 osd
ceph-volume lvm create --data /dev/sdc1
```


回到监控节点: 
运行 `ceph -s `   
可以看到输出中已经有了 osd,这里在三台机器上部署了 ceph-volume,因此创建了三个 osd  

```
[root@cloud1 ceph]# ceph -s
  cluster:
    id:     a31d0c20-a7f0-43ab-92ca-8a4a7d88ef68
    health: HEALTH_WARN
            1 monitors have not enabled msgr2


  services:
    mon: 1 daemons, quorum cloud1 (age 2h)
    mgr: cloud1(active, since 6h)
    osd: 3 osds: 3 up (since 13m), 3 in (since 13m)


  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   3.0 GiB used, 279 GiB / 282 GiB avail
    pgs:
```


# 四、部署元数据服务器 ceph-mds

## 1. 创建 keyring 文件夹  
这里的 id 不可以是纯数字，可以使用主机名,例如 `cloud1`
`mkdir -p /var/lib/ceph/mds/{cluster-name}-{id}`  
示例：
`mkdir -p /var/lib/ceph/mds/ceph-cloud1`
## 2. 创建 keyring 
默认集群名是 ceph,id 是上面的文件夹名中 id 
`sudo -s ceph ceph-authtool --create-keyring /var/lib/ceph/mds/{cluster-name}-{id}/keyring --gen-key -n mds.{id}`
## 3. 将 keyring 添加到 mon 中 
通过密钥认证监控节点  
`ceph auth add mds.{id} osd "allow rwx" mds "allow" mon "allow profile mds" -i /var/lib/ceph/mds/{cluster}-{id}/keyring`


## 4. 启动 ceph-mds 服务
需要保证 /var/lib/ceph/mds/{cluster}-{id}/keyring 文件的所有者和所有者组是 ceph:ceph，如果不是，可以通过 `chown ceph:ceph /var/lib/ceph/mds/{cluster}-{id}/keyring` 来修改  
`systemctl start ceph-mds@{id}`
此处的 id 为上面的id，应该是主机名

示例：
`systemctl start ceph-mds@cloud1`


或者使用手动方法（不推荐）：   
`ceph-mds --cluster {cluster-name} -i {id} -m {mon-hostname}:{mon-port} [-f]`  





# 五、到此完成
** 下面内容不需要 **
---------

# 六、创建 ceph fs system
## 5.1 创建 osd pool 
一个 pool 存数据，一个存元数据    


pgnum 推荐64 、128 
TODO 含义？？？
> Generally, the metadata pool will have at most a few gigabytes of data. For this reason, a smaller PG count is usually recommended. 64 or 128 is commonly used in practice for large clusters.


```
[root@cloud1 ~]# ceph osd pool create cephfs_data 64
pool 'cephfs_data' created
[root@cloud1 ~]# ceph osd pool create cephfs_metadata 64
pool 'cephfs_metadata' created


```


## 5.2 创建 ceph fs
```
# ceph fs new <fs_name> <metadata> <data> 
[root@cloud1 ~]# ceph fs new cephfs cephfs_metadata cephfs_data
new fs with metadata pool 2 and data pool 1
# 使用 ceph fs ls 查看
[root@cloud1 ~]# ceph fs ls
name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]
```


## 5.3 挂载 cephfs 到本地目录，测试使用
```
#这里复制了 admin 的密钥 即 /etc/ceph/ceph.client.admin.keyring
mkdir ~/cephfs_dir
mount -t ceph :/ ~/cephfs_dir/ -o name=admin
```
这样就可以通过 cephfs_dir 向cephfs中写文件了




## 5.4  验证安装
向 cephfs_dir 中写入文件   
```
ceph -s 


  cluster:
    id:     a31d0c20-a7f0-43ab-92ca-8a4a7d88ef68
    health: HEALTH_OK
 
  services:
    mon: 1 daemons, quorum cloud1 (age 32m)
    mgr: cloud1(active, since 20h)
    mds: cephfs:1 {0=mds0=up:active} 2 up:standby  # 这里的 0=mds0=up:active 表示第一个 mds0 active 启用了，不是表示没有mds !!
    osd: 3 osds: 3 up (since 21h), 3 in (since 21h)
 
  data:
    pools:   2 pools, 128 pgs
    objects: 141 objects, 80 MiB
    usage:   3.3 GiB used, 278 GiB / 282 GiB avail
    pgs:     128 active+clean
 
  io:
    client:   525 KiB/s wr, 0 op/s rd, 0 op/s wr


```




# 六、ceph 测试性能  
参考下面网址：  

https://ivanzz1001.github.io/records/post/ceph/2017/07/28/ceph-benchmark  

https://juejin.im/entry/5ab347daf265da238d509ed5





openstack 官方生产环境 ceph 集群架构
[https://docs.openstack.org/openstack-ansible/latest/user/ceph/full-deploy.html](https://docs.openstack.org/openstack-ansible/latest/user/ceph/full-deploy.html)